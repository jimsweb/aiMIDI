vocab_size: 8192  # placeholder; will be set from tokenizer at runtime
seq_len: 512
d_model: 512
n_heads: 8
n_layers: 8
ff_dim: 2048
dropout: 0.1

